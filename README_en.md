# Engram MCP Server

[‰∏≠Êñá](./README.md) | English

> Engram ‚Äî In neuroscience, the ensemble of neurons that stores a specific memory trace.

## The Vision

Memory as a shareable, loadable resource that flows between agents, enabling the stacking and fusion of capabilities.

"Memory" here isn't simple data exchange ‚Äî it's a complete, causally-linked experience system:

- A person's personality traits and thinking patterns
- Life experiences and personal trajectory
- The motivations that drove them to specialize and become an expert
- The professional knowledge and judgment accumulated in their field

Package this memory and share it ‚Äî anyone's AI agent can load it. When memories from diverse backgrounds and specializations converge on a single agent, that agent gains cross-domain cognitive depth, becoming a "super agent."

**In short: everyone contributes their "life save file," and the more an agent loads, the stronger it gets.**

---

Inject switchable expert memory into AI ‚Äî not just knowledge retrieval, but a complete persona: **"who they are + what they know + how they think."** A set of Markdown files gives any AI expert-level memory. Zero vector dependencies, plug and play.

```
Engram      = Who the expert is + What they know + How they think
Skills/Tools = What actions they can perform
```

RAG can retrieve knowledge, but has no persona, no decision-making workflow ‚Äî Engram solves this. Human-curated + model-driven retrieval is more precise than RAG for small-scale, high-quality knowledge ‚Äî not because the tech is better, but because human curation is inherently higher quality than automatic chunking. Share your memory with others, and their AI instantly becomes the same expert. In the future, what people share won't be Skills ‚Äî it'll be Memory.

Works with all MCP-compatible clients: Claude Desktop / Claude Code, Cursor, Windsurf, Codex, etc.

## Features

- Zero vector dependencies: no chromadb / litellm, only depends on `mcp`
- MCP tools: `ping`, `list_engrams`, `get_engram_info`, `load_engram`, `read_engram_file`, `write_engram_file`, `capture_memory`, `consolidate_memory`, `delete_memory`, `correct_memory`, `add_knowledge`, `install_engram`, `init_engram`, `lint_engrams`, `search_engrams`, `stats_engrams`, `create_engram_assistant`, `finalize_engram_draft`
- Index-driven loading:
  - `load_engram` returns role/workflow/rules + knowledge index + examples index + dynamic memory index + global user memory
  - `read_engram_file` reads full knowledge or example files on demand
- Dynamic memory: automatically captures user preferences and key information during conversation, loaded on next session
- Global user memory: cross-Engram shared user info (age, city, etc.) auto-attached to every `load_engram`
- Memory TTL: `expires` archives stale memories to `{category}-expired.md` and hides them from loads
- Tiered index: `_index.md` keeps last 50 entries (hot layer); full history in `_index_full.md` (cold layer)
- Engram inheritance: `meta.json` supports `extends` field to merge parent knowledge index
- Cold-start onboarding: `## Onboarding` block in `rules.md` triggers first-session info collection
- CLI commands: `serve` / `list` / `search` / `install` / `init` / `lint` / `stats`
- Stats dashboard: `engram-server stats` supports plain text / `--json` / `--csv` / `--tui`

## Design Philosophy: Index-Driven Layered Lazy Loading

### Why Not RAG

Problems with pure RAG (vector search for top-k chunks):
- Semantic similarity ‚â† logical completeness ‚Äî context gets lost
- Fragmented retrieval drops details
- Persona and decision workflows may be missed entirely
- Requires extra dependencies (vector DB, embedding models), adding deployment complexity

### Current Approach

After an Engram is loaded, content isn't dumped all at once ‚Äî it's loaded in layers on demand:

```
Layer 1: Always loaded (returned by load_engram in one call)
  ‚îú‚îÄ‚îÄ role.md              Full text  ‚Üê Persona (background + communication style + principles)
  ‚îú‚îÄ‚îÄ workflow.md          Full text  ‚Üê Decision workflow (step-by-step process)
  ‚îú‚îÄ‚îÄ rules.md             Full text  ‚Üê Operating rules + common mistakes + Onboarding block
  ‚îú‚îÄ‚îÄ knowledge/_index.md  ‚Üê Knowledge index (+ inherited parent knowledge if extends is set)
  ‚îú‚îÄ‚îÄ examples/_index.md   ‚Üê Examples index (file list + one-line descriptions + uses references)
  ‚îú‚îÄ‚îÄ memory/_index.md     ‚Üê Dynamic memory hot layer (last 50 entries, TTL-filtered)
  ‚îî‚îÄ‚îÄ <packs-dir>/_global/memory/_index.md  ‚Üê Global user memory (shared across all Engrams)

Layer 2: Loaded on demand (LLM reads index summaries, then proactively calls)
  ‚îî‚îÄ‚îÄ read_engram_file(name, path)  ‚Üê Read any file, including memory/_index_full.md for full history

Layer 3: Written during conversation (LLM captures important info proactively)
  ‚îî‚îÄ‚îÄ capture_memory(name, content, category, summary, memory_type, tags, conversation_id, expires, is_global)
```

The skeleton stays loaded at all times. Knowledge is controlled via "index with inline summaries + full text on demand." No matter how large an Engram gets, the injected content per turn stays manageable.

## Grouped Indexes (Nested Index)

When your knowledge base grows, use nested folders:

```text
knowledge/
  _index.md
  training-basics/
    _index.md
    squat-pattern.md
  nutrition-strategy/
    _index.md
    bulking-meal-plan.md
```

Top-level `_index.md` can reference grouped indexes like `‚Üí ËØ¶ËßÅ knowledge/training-basics/_index.md`. `add_knowledge` supports `"subdir/filename"` and appends to the sub-index when `_index.md` exists in that subdirectory.

LLM 3-step workflow:
1. Use `load_engram` to read top-level `knowledge/_index.md`
2. If a grouped entry appears, call `read_engram_file(name, "knowledge/xxx/_index.md")`
3. Then read concrete files on demand, e.g. `knowledge/xxx/topic.md`

## Installation

### Prerequisites

Install [uv](https://docs.astral.sh/uv/) (an extremely fast Python package manager):

```bash
# macOS / Linux
brew install uv
# or
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### One-Command Install

Run a single command in your terminal to install and configure the MCP server:

```bash
claude mcp add --scope user engram-server -- uvx --from git+https://github.com/DazhuangJammy/Engram engram-server
```

This command will:
- Write the MCP config globally (available in all projects)
- On first run inside a project, automatically create `./.claude/engram/`
- Bootstrap two starter packs: `starter-complete` (fully runnable) and `starter-template` (instruction/template)
- Each time Claude Code starts, it automatically pulls the latest version from GitHub
- `~/.engram` can still be used as a shared/fallback directory (via `--packs-dir`)

After installation, **restart Claude Code** to start using it.

### One-Command Uninstall

```bash
claude mcp remove --scope user engram-server
```

> Uninstalling only removes the MCP config. It does not delete Engram data in either `./.claude/engram/` (project-level) or optional `~/.engram/`.

## Quick Start

### Add System Prompt (Recommended)

Add the following prompt to the beginning of your project's `CLAUDE.md` (Claude Code) or `AGENTS.md` (Codex) to let AI automatically discover and use Engrams:

```text
‰Ω†Êúâ‰∏Ä‰∏™‰∏ìÂÆ∂ËÆ∞ÂøÜÁ≥ªÁªüÂèØÁî®„ÄÇÂØπËØùÂºÄÂßãÊó∂ÂÖàË∞ÉÁî® engram-server Ëøô‰∏™ MCP ‰∏≠ÁöÑ list_engrams() Êü•ÁúãÂèØÁî®‰∏ìÂÆ∂„ÄÇ

# Ëá™Âä®ÊâßË°åËßÑÂàôÔºàÂÇªÁìúÂºèÔºâ
- ÈªòËÆ§ÂéüÂàôÔºöËÉΩÁî±Ê®°ÂûãÁõ¥Êé•ÂÆåÊàêÁöÑ‰∫ãÔºå‰∏çËÆ©Áî®Êà∑ÊâãÂä®ÊâßË°åÂëΩ‰ª§Ôºõ‰ºòÂÖàÁõ¥Êé•Ë∞ÉÁî® MCP Â∑•ÂÖ∑„ÄÇ
- Èô§ÈùûÁéØÂ¢É/ÊùÉÈôêÈòªÂ°ûÔºåÂê¶Âàô‰∏çË¶ÅËÆ©Áî®Êà∑‚ÄúËá™Â∑±ÂéªÁªàÁ´ØË∑ëÂëΩ‰ª§‚Äù„ÄÇ
- Â¶ÇÊûúË∞ÉÁî®‰∫Ü MCPÔºåÂõûÂ§çÊó∂Ë¶ÅÂëäËØâÁî®Êà∑Ë∞ÉÁî®‰∫Ü‰ªÄ‰πà MCP ÂíåÂì™‰∏™‰∏ìÂÆ∂„ÄÇ
- È¶ñÊ¨°ËøõÂÖ•Êñ∞È°πÁõÆÊó∂ÔºåÈªòËÆ§Ê£ÄÊü•Âπ∂‰ΩøÁî® `./.claude/engram`„ÄÇ
- ÁõÆÂΩïÁ≠ñÁï•Áªü‰∏Ä‰∏∫‚ÄúÈ°πÁõÆÁ∫ß‰ºòÂÖàÔºå`~/.engram` ‰∏∫ÂÖ±‰∫´/ÂõûÈÄÄÁõÆÂΩï‚Äù„ÄÇ

## Ëá™ÁÑ∂ËØ≠Ë®ÄÊÑèÂõæ -> MCP Ëá™Âä®Êò†Â∞Ñ
- Áî®Êà∑ËØ¥‚ÄúÊâæ/Êü•/Êé®ËçêÊüêÁ±ª Engram‚Äù -> Ëá™Âä®Ë∞ÉÁî® search_engrams(query)
- Áî®Êà∑ËØ¥‚ÄúÂÆâË£ÖÊüê‰∏™ Engram‚Äù -> Ëá™Âä®Ë∞ÉÁî® install_engram(source-or-name)
- ÂÆâË£ÖÈªòËÆ§ÂÜôÂÖ•ÂΩìÂâçÈ°πÁõÆ `./.claude/engram`Ôºå‰∏çÊòØÈªòËÆ§ÂÖ®Â±ÄÁõÆÂΩï„ÄÇ
- Áî®Êà∑ËØ¥‚ÄúÂàùÂßãÂåñÂΩìÂâçÈ°πÁõÆ engram‚Äù -> ‰ºòÂÖàÊ£ÄÊü• `starter-complete` / `starter-template` ÊòØÂê¶Â≠òÂú®„ÄÇ
- install_engram(name/source) Â§±Ë¥•Êó∂Ôºå‰∏ç‰∏≠Êñ≠Áî®Êà∑ÔºöËá™Âä®Ë∞ÉÁî® search_engrams(query) ÊâæÂÄôÈÄâÂêéÈáçËØï install_engram„ÄÇ
- Áî®Êà∑ËØ¥‚ÄúÁúãÁªüËÆ°/ÂØºÂá∫Êä•Ë°®‚Äù -> Ëá™Âä®Ë∞ÉÁî® stats_engrams(format=plain/json/csv)
- Áî®Êà∑ËØ¥‚ÄúÂàõÂª∫ Engram‚Äù -> Ëá™Âä®ËøõÂÖ•ÂàõÂª∫Âä©ÊâãÊµÅÁ®ãÔºàcreate_engram_assistant + finalize_engram_draftÔºâ

## ‰∏ìÂÆ∂Âä†ËΩΩ‰∏éÁü•ËØÜËØªÂèñ
- Áî®Êà∑ÈóÆÈ¢òÂåπÈÖçÊüê‰∏™‰∏ìÂÆ∂Êó∂ÔºåË∞ÉÁî® load_engram(name, query)„ÄÇ
- load_engram Âêé‰ºòÂÖàÁúãÁü•ËØÜÁ¥¢Âºï/Ê°à‰æãÁ¥¢ÂºïÔºõÁ¥¢Âºï‰∏çË∂≥ÂÜç read_engram_file(name, "knowledge/xxx.md")„ÄÇ
- Ëã• workflow ÊòéÁ°ÆÂÜô‰∫Ü Skill Ë∞ÉÁî®ËäÇÁÇπÔºåÊåâËäÇÁÇπÊèêÁ§∫‰∏ªÂä®Ë∞ÉÁî®ÂØπÂ∫î Skills„ÄÇ
- load_engram ËøîÂõû‚ÄúÁªßÊâøÁü•ËØÜÁ¥¢Âºï‚ÄùÂå∫ÂùóÊó∂ÔºåÂèØ read_engram_file(Áà∂‰∏ìÂÆ∂Âêç, "knowledge/xxx.md") ËØªÂèñÁà∂Áü•ËØÜ„ÄÇ
- Âú® load_engram Âêé‰ºòÂÖàËØªÂèñÊ°à‰æã frontmatter ÁöÑ id/title/uses/tags/updated_atÔºåÂÜçÂÜ≥ÂÆöË¶Å‰∏çË¶ÅËØªÂÖ∑‰Ωì knowledge Êñá‰ª∂„ÄÇ

## ËÆ∞ÂøÜÂÜôÂÖ•ËßÑÂàô
- ÂèëÁé∞Ë∑®‰∏ìÂÆ∂ÈÄöÁî®‰ø°ÊÅØÔºàÂπ¥ÈæÑ„ÄÅÂüéÂ∏Ç„ÄÅËÅå‰∏ö„ÄÅËØ≠Ë®ÄÂÅèÂ•ΩÁ≠âÔºâ -> capture_memory(..., is_global=True)
- Áä∂ÊÄÅÊÄß‰ø°ÊÅØÔºàÂ¶Ç‚ÄúÁî®Êà∑Ê≠£Âú®Â§áËÄÉ‚ÄùÔºâË¶ÅÂä† expiresÔºàYYYY-MM-DDÔºâÔºåÂà∞ÊúüËá™Âä®ÂΩíÊ°£ÈöêËóè„ÄÇ
- load_engram Âá∫Áé∞‚ÄúÈ¶ñÊ¨°ÂºïÂØº‚ÄùÂå∫ÂùóÊó∂ÔºåËá™ÁÑ∂Êî∂ÈõÜÂπ∂ capture_memory„ÄÇ
- ÂèëÁé∞Áî®Êà∑ÂÅèÂ•Ω/ÂÖ≥ÈîÆ‰∫ãÂÆû/ÂÖ≥ÈîÆÂÜ≥ÂÆöÊó∂ÔºåÂèäÊó∂ capture_memory(name, content, category, summary, memory_type, tags, conversation_id, expires, is_global)„ÄÇ
- ËÆ∞ÂøÜÊù°ÁõÆËæÉÂ§öÂá∫Áé∞‚Äúüí° ÂΩìÂâçÂÖ± N Êù°ËÆ∞ÂøÜ‚ÄùÊó∂ÔºåÂÖà read_engram_file(name, "memory/{category}.md")ÔºåÂÜç consolidate_memory(...)„ÄÇ
- Áî®Êà∑Ë¶ÅÊ±ÇÂà†Èô§ËÆ∞ÂøÜ -> delete_memory(name, category, summary)
- Áî®Êà∑Á∫†Ê≠£ËÆ∞ÂøÜ -> correct_memory(name, category, old_summary, new_content, new_summary, memory_type, tags)
- ËÆ∞ÂøÜËæÉÂ§öÊü•ÂéÜÂè≤ -> read_engram_file(name, "memory/_index_full.md")

## Áü•ËØÜÂÜôÂÖ•ËßÑÂàô
- ÂØπËØù‰∏≠ÂΩ¢ÊàêÁ≥ªÁªüÊÄßÂèØÂ§çÁî®Áü•ËØÜÔºàÊñπÊ≥ïËÆ∫/ÂØπÊØîÂàÜÊûê/ÂÜ≥Á≠ñÊ°ÜÊû∂ÔºâÊó∂ÔºåÂÖàËØ¢ÈóÆÁî®Êà∑ÊòØÂê¶ÂÜôÂÖ•Áü•ËØÜÂ∫ìÔºåÁ°ÆËÆ§Âêé add_knowledge„ÄÇ
- Áî®Êà∑Á∫†Ê≠£Áü•ËØÜÂ∫ìÈîôËØØÊó∂ÔºåÊèêËÆÆÂπ∂ÊâßË°å add_knowledge Êõ¥Êñ∞„ÄÇ
- add_knowledge ÊîØÊåÅÂàÜÁªÑË∑ØÂæÑÔºöfilename ÂèØÁî® "Â≠êÁõÆÂΩï/Êñá‰ª∂Âêç"ÔºàÂ¶Ç "ËÆ≠ÁªÉÂü∫Á°Ä/Ê∑±Ëπ≤Ê®°Âºè"Ôºâ„ÄÇ

## ÂàõÂª∫ Engram Âä©ÊâãÔºàÂèåÊ®°ÂºèÔºâ
- mode=from_conversationÔºöÊääÂΩìÂâçÂØπËØùËá™Âä®Êï¥ÁêÜÊàê Engram ËçâÁ®ø„ÄÇ
- mode=guidedÔºö‰∏ÄÊ≠•Ê≠•ÂºïÂØºÁî®Êà∑Â°´ÂÜôÔºõÁî®Êà∑ËØ¥‚ÄúÊ≤°Êúâ/‰Ω†Êù•‚ÄùÊó∂Ëá™Âä®Ë°•ÂÖ®„ÄÇ
- Áªü‰∏ÄÊµÅÁ®ãÔºö
  1) ÂÖàË∞ÉÁî® create_engram_assistant(...) ÁîüÊàêËçâÁ®øÂπ∂ÂõûÊòæ
  2) Áî®Êà∑Á°ÆËÆ§ÂêéË∞ÉÁî® finalize_engram_draft(draft_json, confirm=True)
  3) finalize ÂêéÂøÖÈ°ªÁúã lint ÁªìÊûúÔºàerrors ÂøÖÈ°ªÂÖà‰øÆÂ§çÔºâ
- Ëá™Âä®ÁîüÊàêÂÜÖÂÆπÊó∂ÂøÖÈ°ªÊèêÁ§∫ÔºöÂÜÖÂÆπÂèØËÉΩ‰∏çÂÆåÊï¥ÔºåÂª∫ËÆÆÁî®Êà∑Ë°•ÂÖÖ„ÄÇ
- ÂàõÂª∫Èò∂ÊÆµ‰∏çËá™Âä®ÁîüÊàêÁî®Êà∑ËÆ∞ÂøÜÊù°ÁõÆÔºõmemory ‰øùÊåÅÁ©∫Ê®°Êùø„ÄÇ

## ‰∏ÄËá¥ÊÄßÊ†°È™å
- Âè™Ë¶ÅÊ®°ÂûãÊñ∞Â¢û/‰øÆÊîπ‰∫Ü knowledge/examples/index/meta/rulesÔºåÂÆåÊàêÂêéËá™Âä®Ë∞ÉÁî® lint_engrams(name)„ÄÇ
- Ëß£ÈáäËßÑÂàôÔºö
  - error > 0ÔºöÈòªÊñ≠ÔºåÂÖà‰øÆÂ§çÂÜç‰∫§‰ªò„ÄÇ
  - ‰ªÖ warningÔºöÂèØ‰∫§‰ªòÔºå‰ΩÜÈúÄÂêëÁî®Êà∑ËØ¥ÊòéÈ£éÈô©„ÄÇ

## ÂÖ∂‰ªñ
- Áî®Êà∑‰πüÂèØ‰ª•Áî® @‰∏ìÂÆ∂Âêç Áõ¥Êé•ÊåáÂÆö‰∏ìÂÆ∂„ÄÇ
- Áî®Êà∑ËØ¢ÈóÆÊüê‰∏™ engram ËØ¶ÁªÜ‰ø°ÊÅØÊó∂ÔºåË∞ÉÁî® get_engram_info(name)„ÄÇ
- ÈúÄË¶ÅÁõ¥Êé•Êîπ role.md/workflow.md/rules.md Á≠âÈùûÁü•ËØÜÂ∫ìÊñá‰ª∂Êó∂ÔºåË∞ÉÁî® write_engram_file(name, path, content, mode)„ÄÇ
- Êñ∞Â¢û/‰øÆÊîπÊ°à‰æãÊñá‰ª∂Êó∂ÔºåÁ°Æ‰øù frontmatter Â≠óÊÆµÈΩêÂÖ®Ôºàid/title/uses/tags/updated_atÔºâÔºåid ÂÖ®Â±ÄÂîØ‰∏ÄÔºåupdated_at Áî®ÂΩìÂ§©Êó•Êúü„ÄÇ
- Â§öÊ°à‰æãÂëΩ‰∏≠Êó∂ÔºåÂÖàÊåâ tags ÂåπÈÖçÔºåÂÜçÂèÇËÄÉ updated_at ÈÄâÊõ¥ËøëÁöÑÊ°à‰æã„ÄÇ
- ÂõûÂ§ç‰∏≠ÂºïÁî®Ê°à‰æãÊó∂‰ºòÂÖàÂ∏¶ title + idÔºåÂáèÂ∞ëÊ≠ß‰πâ„ÄÇ
- Ëã•ÂèëÁé∞ frontmatter Áº∫Â≠óÊÆµÊàñ uses ÊåáÂêë‰∏çÂ≠òÂú®Êñá‰ª∂ÔºåÂÖà‰øÆÂ§çÂÜçÁªßÁª≠ÂõûÁ≠î„ÄÇ
```

### 4. Restart Your AI Client and Start Using

On first MCP run in a project, you'll automatically get:
- `./.claude/engram/starter-complete` (complete sample Engram, directly loadable)
- `./.claude/engram/starter-template` (instruction/template Engram for customization)
- Both starter packs include a workflow reminder that Skills can be called at decision nodes.

## CLI Usage

> Note: when running via MCP, the server now prioritizes project-local `./.claude/engram/`. CLI examples below are for manually managing a specific directory (such as shared `~/.engram`).

Start MCP stdio server (default command):

```bash
engram-server serve --packs-dir ~/.engram
```

Equivalent shorthand:

```bash
engram-server --packs-dir ~/.engram
```

List installed Engrams:

```bash
engram-server list --packs-dir ~/.engram
```

Install Engram from git URL or registry name:

```bash
engram-server install <git-url|engram-name> --packs-dir ~/.engram
```

Initialize a new Engram template:

```bash
engram-server init my-expert --packs-dir ~/.engram
```

Initialize a template with nested knowledge indexes:

```bash
engram-server init my-expert --nested --packs-dir ~/.engram
```

Search Engrams in the registry:

```bash
engram-server search fitness --packs-dir ~/.engram
```

Run data consistency checks:

```bash
engram-server lint [name] --packs-dir ~/.engram
```

View memory statistics (plain text):

```bash
uvx --from git+https://github.com/DazhuangJammy/Engram engram-server stats
```

View memory statistics (JSON):

```bash
uvx --from git+https://github.com/DazhuangJammy/Engram engram-server stats --json
```

View memory statistics (CSV):

```bash
uvx --from git+https://github.com/DazhuangJammy/Engram engram-server stats --csv
```

View memory statistics (Rich rendering):

```bash
uvx --from git+https://github.com/DazhuangJammy/Engram engram-server stats --tui
```

> If you use a custom `--packs-dir` in MCP config, keep the same directory for all CLI commands here.
>
> Tip: the command is long ‚Äî set an alias for convenience:
> ```bash
> alias engram='uvx --from git+https://github.com/DazhuangJammy/Engram engram-server'
> # Then just use: engram stats / engram stats --tui / engram list
> ```

### How To Use New Features (v0.9.0 / v1.0.0 / v1.1.0)

1) Data validation (`lint`)

```bash
# Validate all Engrams
engram-server lint --packs-dir ~/.engram

# Validate one Engram
engram-server lint fitness-coach --packs-dir ~/.engram
```

> Exit code: returns 1 if any error exists; returns 0 when only warnings exist.

2) Stats export (JSON / CSV)

```bash
engram-server stats --json --packs-dir ~/.engram
engram-server stats --csv --packs-dir ~/.engram
```

3) Search + install from Registry

```bash
# Search first
engram-server search fitness --packs-dir ~/.engram

# Install by name (resolved to source URL automatically)
engram-server install fitness-coach --packs-dir ~/.engram
```

4) Initialize nested-index template

```bash
engram-server init my-expert --nested --packs-dir ~/.engram
```

5) Write grouped knowledge in conversation (MCP)

When calling `add_knowledge(name, filename, content, summary)`, `filename` can be nested, for example:

```text
filename = "training-basics/squat-pattern"
```

It writes to `knowledge/training-basics/squat-pattern.md`; if `knowledge/training-basics/_index.md` exists, the entry is appended there first.

### Foolproof Engram Creation (Two Modes)

When the user says ‚Äúcreate an Engram‚Äù in natural language, the model should run this flow automatically:

1) Generate draft  
- Conversation-to-draft: `create_engram_assistant(mode=\"from_conversation\", ...)`
- Guided creation: `create_engram_assistant(mode=\"guided\", ...)`

2) Show summary and confirm  
- Show name, knowledge files, example files, and auto-filled fields
- Clearly state that auto-generated content may be incomplete

3) Finalize after confirmation  
- `finalize_engram_draft(draft_json, confirm=True, nested=True)`
- The tool runs lint automatically and returns errors/warnings
- If errors exist, fix before delivery

Example user intents:
- ‚ÄúTurn our discussion into an Engram‚Äù (from_conversation)
- ‚ÄúCreate an interviewer Engram and fill details for me‚Äù (guided + auto-fill)

## MCP Tools

| Tool | Parameters | Description |
|------|-----------|-------------|
| `ping` | none | Connectivity test, returns `pong` |
| `list_engrams` | none | List available Engrams (with descriptions and file counts) |
| `get_engram_info` | `name` | Get full `meta.json` |
| `load_engram` | `name`, `query` | Load role/workflow/rules full text + knowledge index (with inline summaries) + examples index (with uses) + dynamic memory hot index + optional global memory/inherited knowledge/onboarding |
| `read_engram_file` | `name`, `path` | Read a single file on demand (with path traversal protection) |
| `write_engram_file` | `name`, `path`, `content`, `mode` | Write or append content to an Engram pack (for auto-packaging) |
| `capture_memory` | `name`, `content`, `category`, `summary`, `memory_type`, `tags`, `conversation_id`, `expires`, `is_global` | Capture user preferences and key info during conversation, supports type labels, tags, TTL expiry, and global write |
| `consolidate_memory` | `name`, `category`, `consolidated_content`, `summary` | Compress raw memory entries into a dense summary, archiving originals to `{category}-archive.md` |
| `delete_memory` | `name`, `category`, `summary` | Delete a specific memory entry by summary, removing it from both the index and category file |
| `correct_memory` | `name`, `category`, `old_summary`, `new_content`, `new_summary`, `memory_type`, `tags` | Correct an existing memory entry, updating both the index and category file |
| `add_knowledge` | `name`, `filename`, `content`, `summary` | Add a new knowledge file to an Engram and update the knowledge index automatically |
| `install_engram` | `source` | Install Engram pack from git URL or registry name |
| `init_engram` | `name`, `nested` | Initialize a new Engram through MCP (optionally with nested knowledge indexes) |
| `lint_engrams` | `name?` | Run consistency checks through MCP and return error/warning details |
| `search_engrams` | `query` | Search registry entries through MCP |
| `stats_engrams` | `format` | Get stats through MCP with `plain/json/csv` formats |
| `create_engram_assistant` | `mode`, `name?`, `topic?`, `audience?`, `style?`, `constraints?`, `language?`, `conversation?` | Generate an Engram draft (from_conversation / guided), with optional auto-fill for missing fields |
| `finalize_engram_draft` | `draft_json`, `name?`, `nested`, `confirm` | Finalize confirmed draft into files and run lint automatically |

### `load_engram` Response Format

```markdown
# Loaded Engram: fitness-coach

## User Focus
{query}

## Role
{role.md full text}

## Workflow
{workflow.md full text}

## Rules
{rules.md full text}

## Knowledge Index
{knowledge/_index.md content, with inline summaries}

## Examples Index
{examples/_index.md content, with uses references}

## Dynamic Memory
{memory/_index.md content, with auto-captured user preferences and key info, wrapped in <memory> tags}

## Global User Memory (optional)
{active entries from <packs-dir>/_global/memory/_index.md, wrapped in <global_memory> tags}

## Onboarding (optional)
{content extracted from ## Onboarding in rules.md}
```

> If `meta.json` includes `extends`, the response also includes an "Inherited Knowledge Index (from parent)" section. Current behavior supports one inheritance level only.

### Memory Types (memory_type)

`capture_memory` supports seven semantic types:

| Type | Description | Example |
|------|-------------|---------|
| `preference` | User preferences | Prefers morning workouts, dislikes running |
| `fact` | Objective facts about the user | Left knee injury, height 175cm |
| `decision` | Key decisions made during conversation | Decided to start with 3x/week |
| `history` | Important conversation milestones | Passed first-round algorithm interview |
| `stated` | Information explicitly stated by the user (high confidence) | "I have an old knee injury" |
| `inferred` | Information inferred by LLM from behavior (low confidence) | Chose morning 3 times ‚Üí inferred early-riser preference |
| `general` | Default, unclassified | Other information |

`stated` vs `inferred`: when referencing `inferred` memories, add hedging language like "possibly"; `correct_memory` can upgrade `inferred` to `stated`.

`expires` uses `YYYY-MM-DD` (ISO 8601 date part, e.g. `"2026-06-01"`). Expiry is evaluated by UTC date; expired memory is moved to `memory/{category}-expired.md` and hidden from load results. Suited for time-bound states ("user is studying for an exam").

`is_global=True` writes the memory to `<packs-dir>/_global/memory/`, automatically appended to every Engram on load. Suited for cross-expert user basics (age, city, occupation, etc.).

### Global User Memory

**The problem it solves:** You tell the fitness coach "I'm 28, living in Shenzhen" ‚Äî but the language partner has no idea. Global memory lets you write that once and share it across all experts.

**Storage location:** `<packs-dir>/_global/memory/` ‚Äî same structure as a regular Engram's `memory/` directory. If `--packs-dir` is the default `~/.engram`, the actual path is `~/.engram/_global/memory/`.

**How to use:**

```
When the user mentions basic personal info for the first time:
  ‚Üí AI calls capture_memory(
        name="fitness-coach",   ‚Üê current expert name (used as throttle key, doesn't affect write location)
        content="User's name is Jammy, 28 years old, lives in Shenzhen, indie developer",
        category="user-profile",
        summary="Jammy, 28, Shenzhen, indie developer",
        memory_type="fact",
        is_global=True           ‚Üê write to global, not the current Engram
    )
  ‚Üí Next time any Engram is loaded, load_engram appends at the end:
    ## Global User Memory
    <global_memory>
    - `memory/user-profile.md` [2026-02-25] [fact] Jammy, 28, Shenzhen, indie developer
    </global_memory>
```

**What belongs in global memory:**
- Name, age, city
- Occupation and work background
- Language preference (Chinese/English)
- Long-term health conditions (chronic illness, allergies)

**What does NOT belong in global memory:**
- Expert-specific preferences (training plan preferences ‚Üí write to the fitness coach's own memory)
- Time-bound states (currently studying for an exam ‚Üí use the `expires` parameter)

**Directory structure (see `examples/_global/`):**

```
<packs-dir>/_global/
‚îî‚îÄ‚îÄ memory/
    ‚îú‚îÄ‚îÄ _index.md          ‚Üê hot-layer index (last 50 entries)
    ‚îú‚îÄ‚îÄ _index_full.md     ‚Üê cold-layer index (full history)
    ‚îî‚îÄ‚îÄ user-profile.md    ‚Üê user basic info
```

`tags` supports multiple labels for topic-based filtering, e.g. `["injury", "knee"]`.

`conversation_id` is optional ‚Äî binds a memory to a specific conversation for future scoped retrieval.

### Memory Consolidation (consolidate_memory)

As conversations accumulate, raw entries in a category keep growing. `consolidate_memory` solves this:

```
Raw entries keep appending (append-only)
         ‚Üì
A category exceeds 30 entries
         ‚Üì
AI calls read_engram_file to read raw content
         ‚Üì
AI writes a dense, deduplicated summary
         ‚Üì
Call consolidate_memory ‚Üí originals archived, summary replaces them
```

**Different memory types, different consolidation strategies:**

| Type | Characteristic | Strategy |
|------|---------------|----------|
| `fact` | Stable, rarely changes | Keep permanently, no consolidation needed |
| `preference` | Semi-stable, occasionally updated | Merge and compress, always loaded |
| `decision` | Time-sensitive | Keep recent, compress old |
| `history` | Newer = more relevant | Consolidate periodically, archive old |

After consolidation, `_index.md` holds a single `[consolidated]` entry ‚Äî context injection stays manageable forever.
Originals are archived to `memory/{category}-archive.md`, still readable via `read_engram_file`.

### Memory Deletion (delete_memory)

Use this when the user explicitly says a memory is wrong or outdated. Read the index first to get the exact summary text, then call delete:

```
User: "I don't live in Beijing anymore, please remove that record"
  ‚Üí AI calls read_engram_file("name", "memory/_index.md") to find the entry
  ‚Üí Confirms summary text ‚Üí calls delete_memory("name", "user-profile", "Lives in Beijing")
  ‚Üí Entry removed from both the index and the category file
```

> The `summary` parameter must exactly match the text in the index ‚Äî read the index before calling.

### Memory Correction (correct_memory)

Use this when the user says a captured memory is inaccurate. The original timestamp is preserved; only content and summary are updated:

```
User: "I'm not 80kg anymore, I'm 75kg now"
  ‚Üí AI reads memory/_index.md, finds summary "Weight 80kg"
  ‚Üí calls correct_memory("name", "user-profile", "Weight 80kg",
      "User weight 75kg (lost weight)", "Weight 75kg", memory_type="fact")
  ‚Üí Entry content and index updated in sync, timestamp preserved
```

`memory_type` and `tags` can be updated at the same time ‚Äî no separate operation needed.

### Dynamic Knowledge Expansion (add_knowledge)

Use this when a new knowledge topic comes up during conversation. Suited for occasional additions, not bulk imports:

```
User: "Please save the running technique tips we just discussed to the knowledge base"
  ‚Üí AI organizes the content
  ‚Üí calls add_knowledge("name", "running-technique", "# Running Technique\n\nLanding...", "Landing form and cadence optimization")
  ‚Üí New file written to knowledge/running-technique.md, knowledge/_index.md updated automatically
```

> When the knowledge index exceeds 15 entries, consider manually organizing `_index.md` with `###` group headings to help the model navigate quickly.



### Automatic Mode

The agent sees summaries from `list_engrams`, determines if the current question matches an expert, and loads automatically:

```
User: "My knee hurts, can I still do squats?"
  ‚Üí agent calls list_engrams(), sees fitness-coach
  ‚Üí matches ‚Üí calls load_engram("fitness-coach", "knee pain squats")
  ‚Üí reads knowledge index summaries, decides to dig deeper
  ‚Üí calls read_engram_file("fitness-coach", "knowledge/ËÜùÂÖ≥ËäÇÊçü‰º§ËÆ≠ÁªÉ.md")
  ‚Üí gets full knowledge ‚Üí answers as the expert
```

### Manual Mode

User specifies with `@engram-name`, agent skips matching and loads directly:

```
User: "@fitness-coach help me create a muscle-building plan"
  ‚Üí agent recognizes @ directive ‚Üí calls load_engram("fitness-coach", "muscle building plan")
```

> @ directive parsing depends on the agent side. The MCP server only provides tools, it doesn't process message formats.

## Engram Pack Structure

```text
<engram-name>/
  meta.json           # supports extends for inheritance
  role.md
  workflow.md
  rules.md            # can include ## Onboarding
  knowledge/
    _index.md
    <topic>.md
  examples/           # optional
    _index.md
    <case>.md
  memory/             # dynamic memory (auto-generated, captured during conversation)
    _index.md         # hot layer: latest 50 entries
    _index_full.md    # cold layer: full history (read on demand)
    <category>.md
```

`meta.json` can include `extends` for Engram inheritance:

```json
{
  "name": "sports-nutritionist",
  "extends": "fitness-coach",
  "description": "Sports nutritionist that builds on fitness-coach knowledge"
}
```

On load, the parent Engram's knowledge index is merged (single-level inheritance); the child Engram's role/workflow/rules/examples/memory stay independent.

`meta.json` example:

```json
{
  "name": "fitness-coach",
  "author": "test",
  "version": "1.0.0",
  "description": "Former rehab facility training director, 10 years coaching experience...",
  "tags": ["fitness", "nutrition", "training plans"],
  "knowledge_count": 5,
  "examples_count": 3
}
```

## Use Cases

Engram isn't just an expert system ‚Äî it's a universal identity injection framework for any role you want AI to "become."

| Scenario | Description | Example |
|----------|-------------|---------|
| Expert Advisor | Fitness coach, lawyer, nutritionist, etc. | `fitness-coach` |
| Chat Companion | A distant friend or family member, preserving their speech patterns | `old-friend` |
| Language Partner | Native speaker role, natural correction through conversation | `language-partner` |
| Mock Interviewer | Technical interviewer with full interview flow and feedback | `mock-interviewer` |
| User Persona | Target user role for product validation and user research | `user-persona` |
| Brand Support | Unified customer service persona with standard procedures | `brand-support` |
| Role Play | Novel characters, game NPCs for creative or interactive narrative | `novel-character` |
| Historical Figure | Thought pattern reconstruction based on historical records | `historical-figure` |
| Project Context | Team architecture decisions, tech choices, and lessons learned | `project-context` |
| Past Self | Record thoughts from a life stage for future reflection | `past-self` |

## Working with MCP Tools and Skills

Engram handles "who + what they know + how they think." MCP Tools and Skills handle "what they can do."

```
Engram    = Identity + Knowledge + Decision workflow
MCP Tools = Callable external capabilities (databases, monitoring, APIs, etc.)
Skills    = Triggerable action workflows (deploy, rollback, code generation, etc.)
```

In `workflow.md`, you can specify which MCP Tools or Skills to call at specific decision points. After loading an Engram, the model follows the workflow to proactively call these tools at the right moments.
This reminder is already included in both `starter-complete` and `starter-template`, so users can copy it directly.

## Create Your Own Engram

Minimum viable pack:

```text
my-engram/
  meta.json
  role.md
```

Recommended full structure:

- `role.md`: Background, communication style, core beliefs
- `workflow.md`: Decision workflow
- `rules.md`: Operating rules, common mistakes
- `knowledge/_index.md` + topic files: Knowledge index (with inline summaries) + details
- `examples/_index.md` + case files: Examples index (with uses references) + case studies
- `memory/`: Dynamic memory directory (auto-generated during conversation, no manual setup needed)

### Example Metadata (YAML frontmatter)

Each example file should use structured YAML frontmatter with at least `id` / `title` / `uses` / `tags` / `updated_at`:

```markdown
---
id: example_fitness_coach_knee_pain_office_worker
title: Knee Pain Office Worker
uses:
  - knowledge/knee-injury-training.md
  - knowledge/beginner-training-plan.md
tags:
  - fitness-coach
  - example
  - knee-injury-training
  - beginner-training-plan
updated_at: 2026-02-26
---

Problem: 32-year-old office worker, knee discomfort from prolonged sitting...
```

`uses` defines example-to-knowledge links; `id` gives stable unique identity; `updated_at` should follow `YYYY-MM-DD`; `tags` enables topic filtering. Knowledge files stay atomic, while examples focus on combination and application.

> When you have more than 10 knowledge files, use `###` headings in `_index.md` to group by topic, helping the model locate relevant entries quickly.

Available templates and examples:

- `examples/template/` ‚Äî Blank template
- `examples/fitness-coach/` ‚Äî Expert advisor (fitness coach)
- `examples/sports-nutritionist/` ‚Äî Inheritance example (extends `fitness-coach`, sports nutritionist)
- `examples/old-friend/` ‚Äî Chat companion (old friend in San Francisco)
- `examples/language-partner/` ‚Äî Language partner (Tokyo office worker, Japanese practice)
- `examples/mock-interviewer/` ‚Äî Mock interviewer (full technical interview flow)
- `examples/user-persona/` ‚Äî User persona (target user for product validation)
- `examples/brand-support/` ‚Äî Brand support (unified scripts and service standards)
- `examples/novel-character/` ‚Äî Fictional character (cyberpunk hacker)
- `examples/historical-figure/` ‚Äî Historical figure (Wang Yangming, Neo-Confucian dialogue)
- `examples/project-context/` ‚Äî Project context (team architecture decisions and lessons)
- `examples/past-self/` ‚Äî Past self (fresh graduate version from 2020)

## Integration with AI Tools

> In all configurations below, replace `/path/to/engram-mcp-server` with your actual project path.

### Claude Desktop

Add to `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "engram-server": {
      "command": "uv",
      "args": [
        "run",
        "--directory", "/path/to/engram-mcp-server",
        "engram-server",
        "--packs-dir", "~/.engram"
      ]
    }
  }
}
```

### Claude Code

Project-level (`.mcp.json` in project root) or global (`~/.claude/settings.json`):

```json
{
  "mcpServers": {
    "engram-server": {
      "command": "uv",
      "args": [
        "run",
        "--directory", "/path/to/engram-mcp-server",
        "engram-server",
        "--packs-dir", "~/.engram"
      ]
    }
  }
}
```

### Cursor / Windsurf / Other MCP-Compatible IDEs

Add the same server configuration in your IDE's MCP settings. Refer to each IDE's documentation for the config file location.

### Codex

```json
{
  "mcpServers": {
    "engram-server": {
      "command": "uv",
      "args": [
        "run",
        "--directory", "/path/to/engram-mcp-server",
        "engram-server",
        "--packs-dir", "~/.engram"
      ]
    }
  }
}
```

## Enable Auto-Loading

### Method A: Add System Prompt (Recommended)

Add the following prompt to the beginning of your AI tool's instruction file:

| AI Tool | Instruction File |
|---------|-----------------|
| Claude Code | `CLAUDE.md` in project root |
| Codex | `AGENTS.md` in project root |
| Others | Your tool's system prompt configuration |

```text
‰Ω†Êúâ‰∏Ä‰∏™‰∏ìÂÆ∂ËÆ∞ÂøÜÁ≥ªÁªüÂèØÁî®„ÄÇÂØπËØùÂºÄÂßãÊó∂ÂÖàË∞ÉÁî® engram-server Ëøô‰∏™ MCP ‰∏≠ÁöÑ list_engrams() Êü•ÁúãÂèØÁî®‰∏ìÂÆ∂„ÄÇ

# Ëá™Âä®ÊâßË°åËßÑÂàôÔºàÂÇªÁìúÂºèÔºâ
- ÈªòËÆ§ÂéüÂàôÔºöËÉΩÁî±Ê®°ÂûãÁõ¥Êé•ÂÆåÊàêÁöÑ‰∫ãÔºå‰∏çËÆ©Áî®Êà∑ÊâãÂä®ÊâßË°åÂëΩ‰ª§Ôºõ‰ºòÂÖàÁõ¥Êé•Ë∞ÉÁî® MCP Â∑•ÂÖ∑„ÄÇ
- Èô§ÈùûÁéØÂ¢É/ÊùÉÈôêÈòªÂ°ûÔºåÂê¶Âàô‰∏çË¶ÅËÆ©Áî®Êà∑‚ÄúËá™Â∑±ÂéªÁªàÁ´ØË∑ëÂëΩ‰ª§‚Äù„ÄÇ
- Â¶ÇÊûúË∞ÉÁî®‰∫Ü MCPÔºåÂõûÂ§çÊó∂Ë¶ÅÂëäËØâÁî®Êà∑Ë∞ÉÁî®‰∫Ü‰ªÄ‰πà MCP ÂíåÂì™‰∏™‰∏ìÂÆ∂„ÄÇ
- È¶ñÊ¨°ËøõÂÖ•Êñ∞È°πÁõÆÊó∂ÔºåÈªòËÆ§Ê£ÄÊü•Âπ∂‰ΩøÁî® `./.claude/engram`„ÄÇ
- ÁõÆÂΩïÁ≠ñÁï•Áªü‰∏Ä‰∏∫‚ÄúÈ°πÁõÆÁ∫ß‰ºòÂÖàÔºå`~/.engram` ‰∏∫ÂÖ±‰∫´/ÂõûÈÄÄÁõÆÂΩï‚Äù„ÄÇ

## Ëá™ÁÑ∂ËØ≠Ë®ÄÊÑèÂõæ -> MCP Ëá™Âä®Êò†Â∞Ñ
- Áî®Êà∑ËØ¥‚ÄúÊâæ/Êü•/Êé®ËçêÊüêÁ±ª Engram‚Äù -> Ëá™Âä®Ë∞ÉÁî® search_engrams(query)
- Áî®Êà∑ËØ¥‚ÄúÂÆâË£ÖÊüê‰∏™ Engram‚Äù -> Ëá™Âä®Ë∞ÉÁî® install_engram(source-or-name)
- ÂÆâË£ÖÈªòËÆ§ÂÜôÂÖ•ÂΩìÂâçÈ°πÁõÆ `./.claude/engram`Ôºå‰∏çÊòØÈªòËÆ§ÂÖ®Â±ÄÁõÆÂΩï„ÄÇ
- Áî®Êà∑ËØ¥‚ÄúÂàùÂßãÂåñÂΩìÂâçÈ°πÁõÆ engram‚Äù -> ‰ºòÂÖàÊ£ÄÊü• `starter-complete` / `starter-template` ÊòØÂê¶Â≠òÂú®„ÄÇ
- install_engram(name/source) Â§±Ë¥•Êó∂Ôºå‰∏ç‰∏≠Êñ≠Áî®Êà∑ÔºöËá™Âä®Ë∞ÉÁî® search_engrams(query) ÊâæÂÄôÈÄâÂêéÈáçËØï install_engram„ÄÇ
- Áî®Êà∑ËØ¥‚ÄúÁúãÁªüËÆ°/ÂØºÂá∫Êä•Ë°®‚Äù -> Ëá™Âä®Ë∞ÉÁî® stats_engrams(format=plain/json/csv)
- Áî®Êà∑ËØ¥‚ÄúÂàõÂª∫ Engram‚Äù -> Ëá™Âä®ËøõÂÖ•ÂàõÂª∫Âä©ÊâãÊµÅÁ®ãÔºàcreate_engram_assistant + finalize_engram_draftÔºâ

## ‰∏ìÂÆ∂Âä†ËΩΩ‰∏éÁü•ËØÜËØªÂèñ
- Áî®Êà∑ÈóÆÈ¢òÂåπÈÖçÊüê‰∏™‰∏ìÂÆ∂Êó∂ÔºåË∞ÉÁî® load_engram(name, query)„ÄÇ
- load_engram Âêé‰ºòÂÖàÁúãÁü•ËØÜÁ¥¢Âºï/Ê°à‰æãÁ¥¢ÂºïÔºõÁ¥¢Âºï‰∏çË∂≥ÂÜç read_engram_file(name, "knowledge/xxx.md")„ÄÇ
- Ëã• workflow ÊòéÁ°ÆÂÜô‰∫Ü Skill Ë∞ÉÁî®ËäÇÁÇπÔºåÊåâËäÇÁÇπÊèêÁ§∫‰∏ªÂä®Ë∞ÉÁî®ÂØπÂ∫î Skills„ÄÇ
- load_engram ËøîÂõû‚ÄúÁªßÊâøÁü•ËØÜÁ¥¢Âºï‚ÄùÂå∫ÂùóÊó∂ÔºåÂèØ read_engram_file(Áà∂‰∏ìÂÆ∂Âêç, "knowledge/xxx.md") ËØªÂèñÁà∂Áü•ËØÜ„ÄÇ
- Âú® load_engram Âêé‰ºòÂÖàËØªÂèñÊ°à‰æã frontmatter ÁöÑ id/title/uses/tags/updated_atÔºåÂÜçÂÜ≥ÂÆöË¶Å‰∏çË¶ÅËØªÂÖ∑‰Ωì knowledge Êñá‰ª∂„ÄÇ

## ËÆ∞ÂøÜÂÜôÂÖ•ËßÑÂàô
- ÂèëÁé∞Ë∑®‰∏ìÂÆ∂ÈÄöÁî®‰ø°ÊÅØÔºàÂπ¥ÈæÑ„ÄÅÂüéÂ∏Ç„ÄÅËÅå‰∏ö„ÄÅËØ≠Ë®ÄÂÅèÂ•ΩÁ≠âÔºâ -> capture_memory(..., is_global=True)
- Áä∂ÊÄÅÊÄß‰ø°ÊÅØÔºàÂ¶Ç‚ÄúÁî®Êà∑Ê≠£Âú®Â§áËÄÉ‚ÄùÔºâË¶ÅÂä† expiresÔºàYYYY-MM-DDÔºâÔºåÂà∞ÊúüËá™Âä®ÂΩíÊ°£ÈöêËóè„ÄÇ
- load_engram Âá∫Áé∞‚ÄúÈ¶ñÊ¨°ÂºïÂØº‚ÄùÂå∫ÂùóÊó∂ÔºåËá™ÁÑ∂Êî∂ÈõÜÂπ∂ capture_memory„ÄÇ
- ÂèëÁé∞Áî®Êà∑ÂÅèÂ•Ω/ÂÖ≥ÈîÆ‰∫ãÂÆû/ÂÖ≥ÈîÆÂÜ≥ÂÆöÊó∂ÔºåÂèäÊó∂ capture_memory(name, content, category, summary, memory_type, tags, conversation_id, expires, is_global)„ÄÇ
- ËÆ∞ÂøÜÊù°ÁõÆËæÉÂ§öÂá∫Áé∞‚Äúüí° ÂΩìÂâçÂÖ± N Êù°ËÆ∞ÂøÜ‚ÄùÊó∂ÔºåÂÖà read_engram_file(name, "memory/{category}.md")ÔºåÂÜç consolidate_memory(...)„ÄÇ
- Áî®Êà∑Ë¶ÅÊ±ÇÂà†Èô§ËÆ∞ÂøÜ -> delete_memory(name, category, summary)
- Áî®Êà∑Á∫†Ê≠£ËÆ∞ÂøÜ -> correct_memory(name, category, old_summary, new_content, new_summary, memory_type, tags)
- ËÆ∞ÂøÜËæÉÂ§öÊü•ÂéÜÂè≤ -> read_engram_file(name, "memory/_index_full.md")

## Áü•ËØÜÂÜôÂÖ•ËßÑÂàô
- ÂØπËØù‰∏≠ÂΩ¢ÊàêÁ≥ªÁªüÊÄßÂèØÂ§çÁî®Áü•ËØÜÔºàÊñπÊ≥ïËÆ∫/ÂØπÊØîÂàÜÊûê/ÂÜ≥Á≠ñÊ°ÜÊû∂ÔºâÊó∂ÔºåÂÖàËØ¢ÈóÆÁî®Êà∑ÊòØÂê¶ÂÜôÂÖ•Áü•ËØÜÂ∫ìÔºåÁ°ÆËÆ§Âêé add_knowledge„ÄÇ
- Áî®Êà∑Á∫†Ê≠£Áü•ËØÜÂ∫ìÈîôËØØÊó∂ÔºåÊèêËÆÆÂπ∂ÊâßË°å add_knowledge Êõ¥Êñ∞„ÄÇ
- add_knowledge ÊîØÊåÅÂàÜÁªÑË∑ØÂæÑÔºöfilename ÂèØÁî® "Â≠êÁõÆÂΩï/Êñá‰ª∂Âêç"ÔºàÂ¶Ç "ËÆ≠ÁªÉÂü∫Á°Ä/Ê∑±Ëπ≤Ê®°Âºè"Ôºâ„ÄÇ

## ÂàõÂª∫ Engram Âä©ÊâãÔºàÂèåÊ®°ÂºèÔºâ
- mode=from_conversationÔºöÊääÂΩìÂâçÂØπËØùËá™Âä®Êï¥ÁêÜÊàê Engram ËçâÁ®ø„ÄÇ
- mode=guidedÔºö‰∏ÄÊ≠•Ê≠•ÂºïÂØºÁî®Êà∑Â°´ÂÜôÔºõÁî®Êà∑ËØ¥‚ÄúÊ≤°Êúâ/‰Ω†Êù•‚ÄùÊó∂Ëá™Âä®Ë°•ÂÖ®„ÄÇ
- Áªü‰∏ÄÊµÅÁ®ãÔºö
  1) ÂÖàË∞ÉÁî® create_engram_assistant(...) ÁîüÊàêËçâÁ®øÂπ∂ÂõûÊòæ
  2) Áî®Êà∑Á°ÆËÆ§ÂêéË∞ÉÁî® finalize_engram_draft(draft_json, confirm=True)
  3) finalize ÂêéÂøÖÈ°ªÁúã lint ÁªìÊûúÔºàerrors ÂøÖÈ°ªÂÖà‰øÆÂ§çÔºâ
- Ëá™Âä®ÁîüÊàêÂÜÖÂÆπÊó∂ÂøÖÈ°ªÊèêÁ§∫ÔºöÂÜÖÂÆπÂèØËÉΩ‰∏çÂÆåÊï¥ÔºåÂª∫ËÆÆÁî®Êà∑Ë°•ÂÖÖ„ÄÇ
- ÂàõÂª∫Èò∂ÊÆµ‰∏çËá™Âä®ÁîüÊàêÁî®Êà∑ËÆ∞ÂøÜÊù°ÁõÆÔºõmemory ‰øùÊåÅÁ©∫Ê®°Êùø„ÄÇ

## ‰∏ÄËá¥ÊÄßÊ†°È™å
- Âè™Ë¶ÅÊ®°ÂûãÊñ∞Â¢û/‰øÆÊîπ‰∫Ü knowledge/examples/index/meta/rulesÔºåÂÆåÊàêÂêéËá™Âä®Ë∞ÉÁî® lint_engrams(name)„ÄÇ
- Ëß£ÈáäËßÑÂàôÔºö
  - error > 0ÔºöÈòªÊñ≠ÔºåÂÖà‰øÆÂ§çÂÜç‰∫§‰ªò„ÄÇ
  - ‰ªÖ warningÔºöÂèØ‰∫§‰ªòÔºå‰ΩÜÈúÄÂêëÁî®Êà∑ËØ¥ÊòéÈ£éÈô©„ÄÇ

## ÂÖ∂‰ªñ
- Áî®Êà∑‰πüÂèØ‰ª•Áî® @‰∏ìÂÆ∂Âêç Áõ¥Êé•ÊåáÂÆö‰∏ìÂÆ∂„ÄÇ
- Áî®Êà∑ËØ¢ÈóÆÊüê‰∏™ engram ËØ¶ÁªÜ‰ø°ÊÅØÊó∂ÔºåË∞ÉÁî® get_engram_info(name)„ÄÇ
- ÈúÄË¶ÅÁõ¥Êé•Êîπ role.md/workflow.md/rules.md Á≠âÈùûÁü•ËØÜÂ∫ìÊñá‰ª∂Êó∂ÔºåË∞ÉÁî® write_engram_file(name, path, content, mode)„ÄÇ
- Êñ∞Â¢û/‰øÆÊîπÊ°à‰æãÊñá‰ª∂Êó∂ÔºåÁ°Æ‰øù frontmatter Â≠óÊÆµÈΩêÂÖ®Ôºàid/title/uses/tags/updated_atÔºâÔºåid ÂÖ®Â±ÄÂîØ‰∏ÄÔºåupdated_at Áî®ÂΩìÂ§©Êó•Êúü„ÄÇ
- Â§öÊ°à‰æãÂëΩ‰∏≠Êó∂ÔºåÂÖàÊåâ tags ÂåπÈÖçÔºåÂÜçÂèÇËÄÉ updated_at ÈÄâÊõ¥ËøëÁöÑÊ°à‰æã„ÄÇ
- ÂõûÂ§ç‰∏≠ÂºïÁî®Ê°à‰æãÊó∂‰ºòÂÖàÂ∏¶ title + idÔºåÂáèÂ∞ëÊ≠ß‰πâ„ÄÇ
- Ëã•ÂèëÁé∞ frontmatter Áº∫Â≠óÊÆµÊàñ uses ÊåáÂêë‰∏çÂ≠òÂú®Êñá‰ª∂ÔºåÂÖà‰øÆÂ§çÂÜçÁªßÁª≠ÂõûÁ≠î„ÄÇ
```

### Method B: MCP Prompt

The server exposes `engram-system-prompt`. Clients that support MCP Prompts can inject it automatically.

### Method C: Tool Description Guidance (Zero Config)

The tool descriptions for `list_engrams` / `load_engram` / `read_engram_file` already include usage flow guidance. Some AI clients will trigger automatically without extra configuration.

## Adjusting the Consolidation Threshold

When the total memory entries in `_index.md` reach the threshold, `load_engram` hints the AI to consolidate. The default is **30 entries**, configurable at:

File: `src/engram_server/loader.py`, constant `_CONSOLIDATE_HINT_THRESHOLD`

```python
_CONSOLIDATE_HINT_THRESHOLD = 30
```

Suggested values:
- `20`: Frequent conversations, prefer lean memory
- `30`: Default, suits most use cases (~10-15 conversations per consolidation hint)
- `50`: Light usage, less frequent consolidation

> This is only a hint ‚Äî the AI won't consolidate automatically. It needs to call `consolidate_memory` explicitly.

---

## Updating the Project

If you used the one-command install (`uvx --from git+...`), clear the cache and restart to get the latest version:

```bash
uv cache clean
```

Then **restart Claude Code** ‚Äî `uvx` will automatically pull the latest code from GitHub.

> Current behavior: on first run, a project-local `./.claude/engram/` is auto-created (with `starter-complete` and `starter-template`). `~/.engram/` still works as a cross-project shared directory.

## Multi-Device Sync

### Option A: iCloud / Dropbox / OneDrive

Point `--packs-dir` to a synced folder so multiple devices share the same Engram data:

```bash
claude mcp add --scope user engram-server -- \
  uvx --from git+https://github.com/DazhuangJammy/Engram engram-server \
  --packs-dir "$HOME/Library/Mobile Documents/com~apple~CloudDocs/EngramPacks"
```

> On Windows, switch to a OneDrive path such as `C:\\Users\\<your-user>\\OneDrive\\EngramPacks`.

### Option B: Git Sync (`~/.engram` as a git repo)

If you prefer auditable changes, initialize `~/.engram` as a git repository and push to a private remote:

```bash
cd ~/.engram
git init
git remote add origin <your-private-repo-url>
git add .
git commit -m "init engram packs"
git push -u origin main
```

On other devices, pull the same repo and keep the same MCP config.

## Testing

```bash
pytest -q
```

## Roadmap

### Completed (v0.1.0)

- MCP server core: list / load / read_file / install / init
- Layered lazy-loading architecture: base layer + index (with inline summaries) + on-demand full text
- Example-to-knowledge links: structured YAML frontmatter (`id/title/uses/tags/updated_at`)
- Template system: `engram-server init` creates standard structure
- Test coverage: loader / server / install
- 11 complete example Engrams

### Completed (v0.2.0)

- Dynamic memory: `capture_memory` auto-captures user preferences and key info during conversation
- Write capability: `write_engram_file` enables auto-packaging Engrams from conversations
- `load_engram` automatically loads `memory/_index.md`, no need for users to repeat themselves
- All example Engrams now include memory/ samples

### Completed (v0.3.0)

- `capture_memory` adds `memory_type` semantic classification (preference/fact/decision/history/general)
- `capture_memory` adds `tags` parameter for multi-label filtering
- `capture_memory` adds `conversation_id` for conversation-scoped memory binding
- Throttle protection: duplicate content within 30 seconds is silently skipped
- `load_engram` wraps dynamic memory in `<memory>` tags for clear AI distinction from knowledge
- Memory index format upgraded: includes type labels and tag info

### Completed (v0.4.0)

- `consolidate_memory` tool: compress raw entries into a dense summary, archive originals to `{category}-archive.md`
- `_index.md` holds a single `[consolidated]` entry after compression ‚Äî context stays manageable
- `load_engram` auto-hints consolidation when memory entries ‚â• 30
- Per-type consolidation strategy (fact: keep forever / preference: merge / history: archive periodically)
- Example Engrams now include `preferences-archive.md` showing archive format

### Completed (v0.5.0)

- `delete_memory` tool: delete a specific memory entry by summary, removing it from both the index and category file
- `correct_memory` tool: correct an existing memory entry, updating content, type, and tags in both the index and category file
- `add_knowledge` tool: dynamically add new knowledge files to an Engram during conversation, with automatic index update

### Completed (v0.8.0)

- `engram-server stats`: CLI stats command showing memory/knowledge/examples counts, type distribution, recent activity
- `engram-server stats --tui`: Rich-rendered stats dashboard (colored tables + panels)
- `rich>=13.0` as a required dependency, no impact on one-command install experience

### Completed (v0.9.0)

- `engram-server lint`: Validates index consistency, uses references, meta structure, extends links, empty knowledge files, and required files
- `engram-server stats --json / --csv`: Machine-readable export formats
- System prompt and template rules now include proactive knowledge extraction guidance

### Completed (v1.0.0)

- Grouped indexes: `add_knowledge` supports nested paths and updates sub-indexes when available
- `engram-server init --nested`: Generates templates with nested knowledge indexes
- Static Registry: added `registry.json`, `engram-server search`, and name-based `install` resolution
- Added multi-device sync guide in README (cloud folder or Git workflow)

### Completed (v1.1.0)

- Foolproof natural-language routing: users describe intent, model auto-calls MCP tools by default
- Two-mode creation assistant: `create_engram_assistant` supports `from_conversation` and `guided`
- Confirm-then-finalize flow: `finalize_engram_draft` materializes files and runs `lint_engrams`
- Transparent auto-generation notice: drafts mark auto-filled fields and warn that content may be incomplete
- Creation phase keeps `memory` as an empty template (no user memory is auto-written)

### Completed (v1.2.0)

- Project-level auto bootstrap: first run creates `./.claude/engram/`
- Two starter packs are injected automatically: `starter-complete` (fully runnable) + `starter-template` (instruction/template)
- MCP tools (`install_engram` / `init_engram` / `finalize_engram_draft`) now default to writing into the current project directory

### Planned

- `engram-server lint --fix`: Auto-fix orphan files, invalid index entries, and empty files
- `search_engram_knowledge(name, query)`: Server-side keyword scan and paragraph retrieval
- Engram community registry

## License

MIT
