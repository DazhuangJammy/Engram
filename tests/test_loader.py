import json
import re
from pathlib import Path

from engram_server.loader import EngramLoader


FIXTURES_DIR = Path(__file__).parent / "fixtures"
EXAMPLES_DIR = Path(__file__).resolve().parents[1] / "examples"


def _concrete_example_dirs() -> list[Path]:
    """Return non-template examples that should be fully runnable."""
    return sorted(
        d for d in EXAMPLES_DIR.iterdir()
        if d.is_dir() and not d.name.startswith("_") and d.name != "template"
    )


def test_list_engrams_returns_expected_items() -> None:
    loader = EngramLoader(FIXTURES_DIR)

    items = loader.list_engrams()
    names = {item["name"] for item in items}

    assert {"fitness-coach", "contract-lawyer"}.issubset(names)


def test_get_engram_info_returns_meta() -> None:
    loader = EngramLoader(FIXTURES_DIR)

    info = loader.get_engram_info("fitness-coach")

    assert info is not None
    assert info["name"] == "fitness-coach"
    assert "10å¹´æ•™ç»ƒç»éªŒ" in info["description"]
    assert info["knowledge_count"] == 5
    assert info["examples_count"] == 3


def test_load_file_supports_nested_paths() -> None:
    loader = EngramLoader(FIXTURES_DIR)

    role = loader.load_file("fitness-coach", "role.md")
    knowledge = loader.load_file("fitness-coach", "knowledge/å¢žè‚Œè®­ç»ƒåŸºç¡€.md")

    assert role is not None
    assert "ä¸“ä¸šå¥èº«æ•™ç»ƒ" in role
    assert knowledge is not None
    assert "æ¸è¿›è¶…è´Ÿè·" in knowledge


def test_load_file_returns_none_for_invalid_or_missing_file() -> None:
    loader = EngramLoader(FIXTURES_DIR)

    missing = loader.load_file("fitness-coach", "knowledge/ä¸å­˜åœ¨.md")
    escaped = loader.load_file("fitness-coach", "../contract-lawyer/meta.json")

    assert missing is None
    assert escaped is None


def test_list_files_returns_markdown_in_subdir() -> None:
    loader = EngramLoader(FIXTURES_DIR)

    files = loader.list_files("fitness-coach", "knowledge")

    assert files
    assert files[0] == "knowledge/_index.md"
    assert "knowledge/å¢žè‚Œè®­ç»ƒåŸºç¡€.md" in files


def test_load_engram_base_returns_role_workflow_rules_and_indexes() -> None:
    loader = EngramLoader(FIXTURES_DIR)

    content = loader.load_engram_base("fitness-coach")

    assert content is not None
    assert "## è§’è‰²" in content
    assert "ä¸“ä¸šå¥èº«æ•™ç»ƒ" in content
    assert "## å·¥ä½œæµç¨‹" in content
    assert "æ˜Žç¡®ç›®æ ‡ä¸Žé™åˆ¶æ¡ä»¶" in content
    assert "## è§„åˆ™" in content
    assert "å¸¸è§é”™è¯¯" in content
    assert "## çŸ¥è¯†ç´¢å¼•" in content
    assert "knowledge/è†å…³èŠ‚æŸä¼¤è®­ç»ƒ.md" in content
    assert "æ‘˜è¦ï¼š" in content
    assert "## æ¡ˆä¾‹ç´¢å¼•" in content
    assert "examples/è†ç›–ç–¼çš„ä¸Šç­æ—.md" in content
    assert "uses:" in content


def test_empty_directory_does_not_raise(tmp_path: Path) -> None:
    loader = EngramLoader(tmp_path)

    assert loader.list_engrams() == []


def test_missing_meta_directory_is_skipped(tmp_path: Path) -> None:
    (tmp_path / "no-meta").mkdir(parents=True)
    (tmp_path / "has-meta").mkdir(parents=True)
    (tmp_path / "has-meta" / "meta.json").write_text(
        '{"name": "has-meta", "description": "ok"}', encoding="utf-8"
    )

    loader = EngramLoader(tmp_path)
    items = loader.list_engrams()

    assert len(items) == 1
    assert items[0]["name"] == "has-meta"


def _make_engram(tmp_path: Path, name: str = "test-expert") -> EngramLoader:
    """Helper to create a minimal engram for write/memory tests."""
    engram_dir = tmp_path / name
    engram_dir.mkdir()
    (engram_dir / "meta.json").write_text(
        f'{{"name": "{name}", "description": "test"}}', encoding="utf-8"
    )
    (engram_dir / "role.md").write_text("# test role", encoding="utf-8")
    return EngramLoader(tmp_path)


def test_write_file_creates_and_overwrites(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    assert loader.write_file("test-expert", "knowledge/topic.md", "v1")
    assert (tmp_path / "test-expert" / "knowledge" / "topic.md").read_text() == "v1"

    assert loader.write_file("test-expert", "knowledge/topic.md", "v2")
    assert (tmp_path / "test-expert" / "knowledge" / "topic.md").read_text() == "v2"


def test_write_file_append_mode(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    loader.write_file("test-expert", "notes.md", "line1\n")
    loader.write_file("test-expert", "notes.md", "line2\n", append=True)

    content = (tmp_path / "test-expert" / "notes.md").read_text()
    assert "line1" in content
    assert "line2" in content


def test_write_file_blocks_path_traversal(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    assert loader.write_file("test-expert", "../escape.md", "bad") is False


def test_capture_memory_creates_entry_and_index(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    ok = loader.capture_memory(
        "test-expert", "ç”¨æˆ·è†ç›–æœ‰æ—§ä¼¤", "user-profile", "è†å…³èŠ‚æ´»åŠ¨åº¦å—é™"
    )
    assert ok is True

    memory_dir = tmp_path / "test-expert" / "memory"
    assert memory_dir.is_dir()

    category_file = memory_dir / "user-profile.md"
    assert category_file.is_file()
    text = category_file.read_text()
    assert "ç”¨æˆ·è†ç›–æœ‰æ—§ä¼¤" in text
    assert "type:general" in text

    index_file = memory_dir / "_index.md"
    assert index_file.is_file()
    index_content = index_file.read_text()
    assert "è†å…³èŠ‚æ´»åŠ¨åº¦å—é™" in index_content
    assert "memory/user-profile.md" in index_content
    assert "[general]" in index_content


def test_capture_memory_with_type_and_tags(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    ok = loader.capture_memory(
        "test-expert",
        "ç”¨æˆ·åå¥½æ™¨ç»ƒï¼Œä¸å–œæ¬¢å¤œé—´è®­ç»ƒ",
        "preferences",
        "åå¥½æ™¨ç»ƒ",
        memory_type="preference",
        tags=["fitness", "schedule"],
    )
    assert ok is True

    category_file = tmp_path / "test-expert" / "memory" / "preferences.md"
    text = category_file.read_text()
    assert "type:preference" in text
    assert "tags:fitness,schedule" in text

    index_content = (tmp_path / "test-expert" / "memory" / "_index.md").read_text()
    assert "[preference]" in index_content
    assert "[fitness,schedule]" in index_content


def test_capture_memory_with_conversation_id(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    ok = loader.capture_memory(
        "test-expert",
        "æœ¬æ¬¡å¯¹è¯å†³å®šä»Ž3x/weekå¼€å§‹",
        "decisions",
        "åˆå§‹è®­ç»ƒé¢‘çŽ‡3æ¬¡/å‘¨",
        memory_type="decision",
        conversation_id="session-abc123",
    )
    assert ok is True

    text = (tmp_path / "test-expert" / "memory" / "decisions.md").read_text()
    assert "conv:session-abc123" in text
    assert "type:decision" in text


def test_capture_tool_trace_creates_structured_memory(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    ok = loader.capture_tool_trace(
        "test-expert",
        tool_name="search_engrams",
        intent="æŸ¥æ‰¾è¥å…»ç›¸å…³ä¸“å®¶",
        result_summary="å‘½ä¸­ 3 ä¸ªå€™é€‰ä¸“å®¶",
        args_summary='query="nutrition"',
        status="ok",
    )
    assert ok is True

    trace_file = tmp_path / "test-expert" / "memory" / "tool-trace.md"
    text = trace_file.read_text(encoding="utf-8")
    assert "tool: search_engrams" in text
    assert "intent: æŸ¥æ‰¾è¥å…»ç›¸å…³ä¸“å®¶" in text
    assert "result: å‘½ä¸­ 3 ä¸ªå€™é€‰ä¸“å®¶" in text
    assert "type:tool_trace" in text

    index_text = (tmp_path / "test-expert" / "memory" / "_index.md").read_text(
        encoding="utf-8"
    )
    assert "`memory/tool-trace.md`" in index_text
    assert "search_engrams [ok]" in index_text


def test_list_recent_memory_summaries_returns_latest_entries(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    for idx in range(3):
        loader.capture_tool_trace(
            "test-expert",
            tool_name="read_engram_file",
            intent=f"è¯»å–çŸ¥è¯†æ–‡ä»¶{idx}",
            result_summary=f"è¯»å–æˆåŠŸ{idx}",
            status="ok",
        )
        loader._throttle_cache.clear()

    latest_two = loader.list_recent_memory_summaries(
        "test-expert", "tool-trace", limit=2
    )
    assert len(latest_two) == 2
    assert "è¯»å–çŸ¥è¯†æ–‡ä»¶0" not in "\n".join(latest_two)
    assert "è¯»å–çŸ¥è¯†æ–‡ä»¶2" in "\n".join(latest_two)


def test_capture_memory_throttle(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    content = "ç”¨æˆ·è†ç›–æœ‰æ—§ä¼¤"

    loader.capture_memory("test-expert", content, "user-profile", "è†å…³èŠ‚æ´»åŠ¨åº¦å—é™")
    loader.capture_memory("test-expert", content, "user-profile", "è†å…³èŠ‚æ´»åŠ¨åº¦å—é™")

    text = (tmp_path / "test-expert" / "memory" / "user-profile.md").read_text()
    # Only one entry should exist (second call throttled)
    assert text.count("ç”¨æˆ·è†ç›–æœ‰æ—§ä¼¤") == 1


def test_consolidate_memory_archives_and_replaces(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    # Add some raw entries first
    loader.capture_memory("test-expert", "åå¥½æ™¨ç»ƒ", "preferences", "å–œæ¬¢æ—©ä¸Šè®­ç»ƒ", memory_type="preference")
    # bypass throttle by using different content
    loader._throttle_cache.clear()
    loader.capture_memory("test-expert", "å®¶æœ‰å“‘é“ƒ", "preferences", "å±…å®¶è®­ç»ƒè®¾å¤‡", memory_type="preference")

    # Consolidate
    ok = loader.consolidate_memory(
        "test-expert",
        "preferences",
        "ã€è®­ç»ƒæ—¶é—´ã€‘åå¥½æ™¨ç»ƒã€‚ã€è®¾å¤‡ã€‘å®¶æœ‰å“‘é“ƒã€‚",
        "è®­ç»ƒåå¥½æ‘˜è¦",
    )
    assert ok is True

    memory_dir = tmp_path / "test-expert" / "memory"

    # Archive file should exist and contain original entries
    archive = memory_dir / "preferences-archive.md"
    assert archive.is_file()
    archive_text = archive.read_text()
    assert "åå¥½æ™¨ç»ƒ" in archive_text
    assert "å®¶æœ‰å“‘é“ƒ" in archive_text

    # Category file should now contain only consolidated content
    cat_text = (memory_dir / "preferences.md").read_text()
    assert "type:consolidated" in cat_text
    assert "ã€è®­ç»ƒæ—¶é—´ã€‘" in cat_text
    assert "type:preference" not in cat_text  # raw entry format gone

    # Index should have exactly one entry for this category
    index_text = (memory_dir / "_index.md").read_text()
    assert index_text.count("`memory/preferences.md`") == 1
    assert "[consolidated]" in index_text
    assert "è®­ç»ƒåå¥½æ‘˜è¦" in index_text


def test_consolidate_memory_multiple_rounds(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    loader.capture_memory("test-expert", "å†…å®¹A", "history", "æ‘˜è¦A", memory_type="history")
    loader.consolidate_memory("test-expert", "history", "ç¬¬ä¸€æ¬¡åŽ‹ç¼©å†…å®¹", "ç¬¬ä¸€æ¬¡åŽ‹ç¼©")
    loader._throttle_cache.clear()
    loader.capture_memory("test-expert", "å†…å®¹B", "history", "æ‘˜è¦B", memory_type="history")
    loader.consolidate_memory("test-expert", "history", "ç¬¬äºŒæ¬¡åŽ‹ç¼©å†…å®¹", "ç¬¬äºŒæ¬¡åŽ‹ç¼©")

    archive = tmp_path / "test-expert" / "memory" / "history-archive.md"
    archive_text = archive.read_text()
    # Both rounds should be in archive
    assert "ç¬¬ä¸€æ¬¡åŽ‹ç¼©å†…å®¹" in archive_text
    assert "å†…å®¹B" in archive_text

    index_text = (tmp_path / "test-expert" / "memory" / "_index.md").read_text()
    assert index_text.count("`memory/history.md`") == 1
    assert "ç¬¬äºŒæ¬¡åŽ‹ç¼©" in index_text


def test_count_memory_entries(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    assert loader.count_memory_entries("test-expert", "preferences") == 0
    loader.capture_memory("test-expert", "å†…å®¹1", "preferences", "æ‘˜è¦1")
    assert loader.count_memory_entries("test-expert", "preferences") == 1
    loader._throttle_cache.clear()
    loader.capture_memory("test-expert", "å†…å®¹2", "preferences", "æ‘˜è¦2")
    assert loader.count_memory_entries("test-expert", "preferences") == 2

    loader.capture_memory(
        "test-expert", "å–œæ¬¢æ—©ä¸Šè®­ç»ƒ", "preferences", "åå¥½æ™¨ç»ƒ"
    )

    base = loader.load_engram_base("test-expert")
    assert base is not None
    assert "## åŠ¨æ€è®°å¿†" in base
    assert "åå¥½æ™¨ç»ƒ" in base


def test_delete_memory_removes_entry_and_index(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    loader.capture_memory("test-expert", "ç”¨æˆ·å·¦è†æ—§ä¼¤", "user-profile", "è†å…³èŠ‚å—é™", memory_type="fact")

    ok = loader.delete_memory("test-expert", "user-profile", "è†å…³èŠ‚å—é™")
    assert ok is True

    index_text = (tmp_path / "test-expert" / "memory" / "_index.md").read_text()
    assert "è†å…³èŠ‚å—é™" not in index_text

    cat_text = (tmp_path / "test-expert" / "memory" / "user-profile.md").read_text()
    assert "ç”¨æˆ·å·¦è†æ—§ä¼¤" not in cat_text


def test_delete_memory_returns_false_for_missing_summary(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    loader.capture_memory("test-expert", "å†…å®¹", "user-profile", "çœŸå®žæ‘˜è¦")

    ok = loader.delete_memory("test-expert", "user-profile", "ä¸å­˜åœ¨çš„æ‘˜è¦")
    assert ok is False


def test_correct_memory_updates_content_and_index(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)

    loader.capture_memory(
        "test-expert", "ç”¨æˆ·ä½“é‡80kg", "user-profile", "ä½“é‡80kg", memory_type="fact"
    )

    ok = loader.correct_memory(
        "test-expert",
        "user-profile",
        "ä½“é‡80kg",
        "ç”¨æˆ·ä½“é‡75kgï¼ˆå·²å‡é‡ï¼‰",
        "ä½“é‡75kg",
        memory_type="fact",
    )
    assert ok is True

    index_text = (tmp_path / "test-expert" / "memory" / "_index.md").read_text()
    assert "ä½“é‡75kg" in index_text
    assert "ä½“é‡80kg" not in index_text

    cat_text = (tmp_path / "test-expert" / "memory" / "user-profile.md").read_text()
    assert "ç”¨æˆ·ä½“é‡75kg" in cat_text
    assert "ç”¨æˆ·ä½“é‡80kg" not in cat_text


def test_correct_memory_returns_false_for_missing_summary(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    loader.capture_memory("test-expert", "å†…å®¹", "user-profile", "çœŸå®žæ‘˜è¦")

    ok = loader.correct_memory(
        "test-expert", "user-profile", "ä¸å­˜åœ¨çš„æ‘˜è¦", "æ–°å†…å®¹", "æ–°æ‘˜è¦"
    )
    assert ok is False


def test_add_knowledge_creates_file_and_updates_index(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    (tmp_path / "test-expert" / "knowledge").mkdir()
    (tmp_path / "test-expert" / "knowledge" / "_index.md").write_text(
        "- `knowledge/existing.md` - å·²æœ‰çŸ¥è¯†\n", encoding="utf-8"
    )

    ok = loader.add_knowledge(
        "test-expert",
        "new-topic",
        "# æ–°çŸ¥è¯†\n\nè¿™æ˜¯æ–°å¢žçš„çŸ¥è¯†å†…å®¹ã€‚",
        "æ–°å¢žä¸»é¢˜çš„æ ¸å¿ƒè¦ç‚¹",
    )
    assert ok is True

    knowledge_file = tmp_path / "test-expert" / "knowledge" / "new-topic.md"
    assert knowledge_file.is_file()
    assert "æ–°å¢žçš„çŸ¥è¯†å†…å®¹" in knowledge_file.read_text()

    index_text = (tmp_path / "test-expert" / "knowledge" / "_index.md").read_text()
    assert "knowledge/new-topic.md" in index_text
    assert "æ–°å¢žä¸»é¢˜çš„æ ¸å¿ƒè¦ç‚¹" in index_text
    assert "å·²æœ‰çŸ¥è¯†" in index_text  # existing entry preserved


def test_add_knowledge_auto_adds_md_extension(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    (tmp_path / "test-expert" / "knowledge").mkdir()
    (tmp_path / "test-expert" / "knowledge" / "_index.md").write_text("", encoding="utf-8")

    loader.add_knowledge("test-expert", "topic-no-ext", "å†…å®¹", "æ‘˜è¦")

    assert (tmp_path / "test-expert" / "knowledge" / "topic-no-ext.md").is_file()


def test_add_knowledge_nested_prefers_subdir_index(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    (tmp_path / "test-expert" / "knowledge" / "è®­ç»ƒåŸºç¡€").mkdir(parents=True)
    (tmp_path / "test-expert" / "knowledge" / "_index.md").write_text(
        "- `knowledge/existing.md` - é¡¶å±‚\n", encoding="utf-8"
    )
    (tmp_path / "test-expert" / "knowledge" / "è®­ç»ƒåŸºç¡€" / "_index.md").write_text(
        "- `knowledge/è®­ç»ƒåŸºç¡€/existing.md` - å­ç›®å½•\n", encoding="utf-8"
    )

    ok = loader.add_knowledge(
        "test-expert",
        "è®­ç»ƒåŸºç¡€/æ·±è¹²æ¨¡å¼",
        "# æ·±è¹²æ¨¡å¼\næ­£æ–‡",
        "åˆ†ç»„çŸ¥è¯†æ¡ç›®",
    )
    assert ok is True

    nested_file = tmp_path / "test-expert" / "knowledge" / "è®­ç»ƒåŸºç¡€" / "æ·±è¹²æ¨¡å¼.md"
    assert nested_file.is_file()
    nested_index = (
        tmp_path / "test-expert" / "knowledge" / "è®­ç»ƒåŸºç¡€" / "_index.md"
    ).read_text(encoding="utf-8")
    top_index = (tmp_path / "test-expert" / "knowledge" / "_index.md").read_text(
        encoding="utf-8"
    )
    assert "knowledge/è®­ç»ƒåŸºç¡€/æ·±è¹²æ¨¡å¼.md" in nested_index
    assert "knowledge/è®­ç»ƒåŸºç¡€/æ·±è¹²æ¨¡å¼.md" not in top_index


def test_add_knowledge_nested_without_subdir_index_falls_back_to_top_index(
    tmp_path: Path,
) -> None:
    loader = _make_engram(tmp_path)
    (tmp_path / "test-expert" / "knowledge" / "è®­ç»ƒåŸºç¡€").mkdir(parents=True)
    (tmp_path / "test-expert" / "knowledge" / "_index.md").write_text(
        "", encoding="utf-8"
    )

    ok = loader.add_knowledge(
        "test-expert",
        "è®­ç»ƒåŸºç¡€/æ ¸å¿ƒç¨³å®š",
        "# æ ¸å¿ƒç¨³å®š\næ­£æ–‡",
        "æœªå»ºå­ç´¢å¼•æ—¶è¿½åŠ åˆ°é¡¶å±‚",
    )
    assert ok is True

    top_index = (tmp_path / "test-expert" / "knowledge" / "_index.md").read_text(
        encoding="utf-8"
    )
    assert "knowledge/è®­ç»ƒåŸºç¡€/æ ¸å¿ƒç¨³å®š.md" in top_index


def test_expired_memory_is_archived_on_load(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    loader.capture_memory(
        "test-expert",
        "è¿™æ˜¯ä¸€ä¸ªä¼šè¿‡æœŸçš„çŠ¶æ€",
        "status",
        "ä¸´æ—¶çŠ¶æ€",
        memory_type="fact",
        expires="2000-01-01",
    )

    loaded = loader.load_engram_base("test-expert")
    assert loaded is not None
    assert "ä¸´æ—¶çŠ¶æ€" not in loaded

    memory_dir = tmp_path / "test-expert" / "memory"
    expired_file = memory_dir / "status-expired.md"
    assert expired_file.is_file()
    assert "è¿™æ˜¯ä¸€ä¸ªä¼šè¿‡æœŸçš„çŠ¶æ€" in expired_file.read_text(encoding="utf-8")

    status_file = memory_dir / "status.md"
    assert "è¿™æ˜¯ä¸€ä¸ªä¼šè¿‡æœŸçš„çŠ¶æ€" not in status_file.read_text(encoding="utf-8")
    assert "ä¸´æ—¶çŠ¶æ€" not in (memory_dir / "_index.md").read_text(encoding="utf-8")


def test_hot_index_contains_category_summaries(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    loader.capture_memory(
        "test-expert", "åå¥½æ™¨ç»ƒ", "preferences", "å–œæ¬¢æ—©æ™¨è®­ç»ƒ", memory_type="preference"
    )
    loader.capture_memory(
        "test-expert", "å®Œæˆç¬¬ä¸€æ¬¡è®­ç»ƒ", "history", "ç¬¬ä¸€æ¬¡è®­ç»ƒå·²å®Œæˆ", memory_type="history"
    )

    index_text = (tmp_path / "test-expert" / "memory" / "_index.md").read_text(encoding="utf-8")
    assert "## åˆ†ç±»æ‘˜è¦" in index_text
    assert "`preferences`" in index_text
    assert "`history`" in index_text
    assert "## æœ€è¿‘è®°å¿†ï¼ˆæœ€å¤š50æ¡ï¼‰" in index_text


def test_consolidation_hint_threshold_is_30(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path)
    for i in range(29):
        loader.capture_memory("test-expert", f"å†…å®¹{i}", "history", f"æ‘˜è¦{i}")

    loaded_29 = loader.load_engram_base("test-expert")
    assert loaded_29 is not None
    assert "ðŸ’¡ å½“å‰å…±" not in loaded_29

    loader.capture_memory("test-expert", "å†…å®¹29", "history", "æ‘˜è¦29")
    loaded_30 = loader.load_engram_base("test-expert")
    assert loaded_30 is not None
    assert "ðŸ’¡ å½“å‰å…± 30 æ¡è®°å¿†" in loaded_30


def test_global_memory_is_loaded_across_engrams(tmp_path: Path) -> None:
    loader = _make_engram(tmp_path, "expert-a")
    _make_engram(tmp_path, "expert-b")

    ok = loader.capture_memory(
        "expert-a",
        "ç”¨æˆ·å±…ä½åœ¨æ·±åœ³",
        "user-profile",
        "å±…ä½åœ°ï¼šæ·±åœ³",
        memory_type="fact",
        is_global=True,
    )
    assert ok is True

    loaded = loader.load_engram_base("expert-b")
    assert loaded is not None
    assert "## å…¨å±€ç”¨æˆ·è®°å¿†" in loaded
    assert "å±…ä½åœ°ï¼šæ·±åœ³" in loaded


def test_loader_reads_from_project_and_global_roots(tmp_path: Path) -> None:
    project_root = tmp_path / "project-engram"
    global_root = tmp_path / "global-engram"
    project_root.mkdir()
    global_root.mkdir()
    _make_engram(project_root, "project-only")
    _make_engram(global_root, "global-only")

    shared_project = project_root / "shared-expert" / "meta.json"
    shared_project.parent.mkdir(parents=True)
    shared_project.write_text(
        '{"name":"shared-expert","description":"project version"}',
        encoding="utf-8",
    )
    (shared_project.parent / "role.md").write_text("project role", encoding="utf-8")

    shared_global = global_root / "shared-expert" / "meta.json"
    shared_global.parent.mkdir(parents=True)
    shared_global.write_text(
        '{"name":"shared-expert","description":"global version"}',
        encoding="utf-8",
    )
    (shared_global.parent / "role.md").write_text("global role", encoding="utf-8")

    loader = EngramLoader(
        packs_dir=[project_root, global_root],
        default_packs_dir=global_root,
    )
    listed = loader.list_engrams()
    names = [item["name"] for item in listed]

    assert {"project-only", "global-only", "shared-expert"}.issubset(set(names))
    assert names.count("shared-expert") == 1
    assert loader.get_engram_info("shared-expert")["description"] == "project version"

    ok = loader.capture_memory(
        "shared-expert",
        "å…¨å±€åå¥½ï¼šä¸­æ–‡å›žå¤",
        "profile",
        "è¯­è¨€åå¥½ï¼šä¸­æ–‡",
        is_global=True,
    )
    assert ok is True
    assert (global_root / "_global" / "memory" / "_index.md").is_file()


def test_load_engram_base_supports_parent_knowledge_inheritance(tmp_path: Path) -> None:
    parent = tmp_path / "parent"
    parent.mkdir()
    (parent / "meta.json").write_text(
        '{"name":"parent","description":"parent"}',
        encoding="utf-8",
    )
    (parent / "role.md").write_text("parent role", encoding="utf-8")
    (parent / "knowledge").mkdir()
    (parent / "knowledge" / "_index.md").write_text(
        "- `knowledge/base.md` - çˆ¶çŸ¥è¯†æ‘˜è¦",
        encoding="utf-8",
    )

    child = tmp_path / "child"
    child.mkdir()
    (child / "meta.json").write_text(
        '{"name":"child","description":"child","extends":"parent"}',
        encoding="utf-8",
    )
    (child / "role.md").write_text("child role", encoding="utf-8")

    loader = EngramLoader(tmp_path)
    loaded = loader.load_engram_base("child")
    assert loaded is not None
    assert "## ç»§æ‰¿çŸ¥è¯†ç´¢å¼•ï¼ˆæ¥è‡ª parentï¼‰" in loaded
    assert "knowledge/base.md" in loaded


def test_onboarding_prompt_only_shows_before_first_memory(tmp_path: Path) -> None:
    engram_dir = tmp_path / "guide-expert"
    engram_dir.mkdir()
    (engram_dir / "meta.json").write_text(
        '{"name":"guide-expert","description":"guide"}',
        encoding="utf-8",
    )
    (engram_dir / "role.md").write_text("role", encoding="utf-8")
    (engram_dir / "rules.md").write_text(
        "# è§„åˆ™\n\n## Onboarding\n- äº†è§£ç”¨æˆ·ç›®æ ‡\n- äº†è§£ç”¨æˆ·çº¦æŸ\n",
        encoding="utf-8",
    )

    loader = EngramLoader(tmp_path)
    first_loaded = loader.load_engram_base("guide-expert")
    assert first_loaded is not None
    assert "## é¦–æ¬¡å¼•å¯¼" in first_loaded

    loader.capture_memory("guide-expert", "ç”¨æˆ·ç›®æ ‡æ˜¯å‡è„‚", "user-profile", "ç›®æ ‡ï¼šå‡è„‚")
    second_loaded = loader.load_engram_base("guide-expert")
    assert second_loaded is not None
    assert "## é¦–æ¬¡å¼•å¯¼" not in second_loaded


def test_all_example_rules_have_onboarding_section() -> None:
    missing = []
    for rules_file in sorted(EXAMPLES_DIR.glob("*/rules.md")):
        content = rules_file.read_text(encoding="utf-8")
        if "## Onboarding" not in content:
            missing.append(str(rules_file))
    assert missing == []


def test_concrete_examples_have_required_core_files() -> None:
    required = (
        "meta.json",
        "role.md",
        "rules.md",
        "workflow.md",
        "knowledge/_index.md",
        "examples/_index.md",
        "memory/_index.md",
    )
    missing: list[str] = []
    for engram_dir in _concrete_example_dirs():
        for rel in required:
            if not (engram_dir / rel).is_file():
                missing.append(f"{engram_dir.name}/{rel}")
    assert missing == []


def test_concrete_example_indexes_only_reference_existing_files() -> None:
    missing_refs: list[str] = []
    for engram_dir in _concrete_example_dirs():
        for index_rel in ("knowledge/_index.md", "examples/_index.md", "memory/_index.md"):
            index_file = engram_dir / index_rel
            content = index_file.read_text(encoding="utf-8")
            for token in re.findall(r"`([^`]+)`", content):
                if not token.startswith(("knowledge/", "examples/", "memory/")):
                    continue
                if not (engram_dir / token).is_file():
                    missing_refs.append(f"{engram_dir.name}:{index_rel}:{token}")
    assert missing_refs == []


def test_concrete_examples_meta_counts_match_real_files() -> None:
    mismatches: list[str] = []
    for engram_dir in _concrete_example_dirs():
        meta = json.loads((engram_dir / "meta.json").read_text(encoding="utf-8"))
        knowledge_count = len([
            f for f in (engram_dir / "knowledge").glob("*.md")
            if f.name != "_index.md"
        ])
        examples_count = len([
            f for f in (engram_dir / "examples").glob("*.md")
            if f.name != "_index.md"
        ])
        if meta.get("knowledge_count") != knowledge_count:
            mismatches.append(
                f"{engram_dir.name}:knowledge_count={meta.get('knowledge_count')} actual={knowledge_count}"
            )
        if meta.get("examples_count") != examples_count:
            mismatches.append(
                f"{engram_dir.name}:examples_count={meta.get('examples_count')} actual={examples_count}"
            )
    assert mismatches == []


def test_each_concrete_example_has_expires_and_confidence_memory_samples() -> None:
    missing_markers: list[str] = []
    for engram_dir in _concrete_example_dirs():
        merged = "\n".join(
            f.read_text(encoding="utf-8")
            for f in sorted((engram_dir / "memory").glob("*.md"))
        )
        for marker in ("expires:", "type:inferred", "type:stated"):
            if marker not in merged:
                missing_markers.append(f"{engram_dir.name}:{marker}")
    assert missing_markers == []
